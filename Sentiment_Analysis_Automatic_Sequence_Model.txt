# -*- coding: utf-8 -*-
"""CIS511_NLP_Project_Transformers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_uVJNWJEq7-4Zbairky4MMjOvCPKxYDV
"""

import functools
import sys
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchtext
import tqdm
import transformers
import pandas as pd
from datasets import Dataset, DatasetDict, load_dataset
from sklearn.preprocessing import LabelEncoder

# from google.colab import drive

# drive.mount("/content/drive")

seed = 0

torch.manual_seed(seed)

df_train = pd.read_csv("train_sentiment_dataset.csv")
df_test = pd.read_csv("test_sentiment_dataset.csv")

print(len(df_train["text"]))

print(df_train)

train_text = []
for i in range(len(df_train["text"])):
    train_text.append(str(df_train["text"][i]))

test_text = []
for i in range(len(df_test["text"])):
    test_text.append(str(df_test["text"][i]))

train_labels = []
for i in range(len(df_train["label"])):
    train_labels.append(str(df_train["label"][i]))

test_labels = []
for i in range(len(df_test["label"])):
    test_labels.append(str(df_test["label"][i]))


# Sample data
train_data = {"text": train_text, "label": train_labels}
test_data = {"text": test_text, "label": test_labels}


label_encoder = LabelEncoder()
train_data["encoded_label"] = label_encoder.fit_transform(train_data["label"])

test_data["encoded_label"] = label_encoder.fit_transform(test_data["label"])

# Create custom datasets
custom_train_dataset = Dataset.from_dict(train_data)
custom_test_dataset = Dataset.from_dict(test_data)

# Create a DatasetDict
custom_dataset = DatasetDict(
    {"train": custom_train_dataset, "test": custom_test_dataset}
)

# Access the train and test splits
train_data = custom_dataset["train"]
test_data = custom_dataset["test"]

# train_data, test_data = datasets.load_dataset('imdb', split=['train', 'test'])

transformer_name = "bert-base-uncased"

tokenizer = transformers.AutoTokenizer.from_pretrained(transformer_name)

tokenizer.tokenize("hello world!")

tokenizer.encode("hello world!")

tokenizer.convert_ids_to_tokens(tokenizer.encode("hello world"))

tokenizer("hello world!")


def tokenize_and_numericalize_data(example, tokenizer):
    ids = tokenizer(example["text"], truncation=True)["input_ids"]
    return {"ids": ids}


train_data = train_data.map(
    tokenize_and_numericalize_data, fn_kwargs={"tokenizer": tokenizer}
)
test_data = test_data.map(
    tokenize_and_numericalize_data, fn_kwargs={"tokenizer": tokenizer}
)

train_data[0]

tokenizer.vocab["!"]

tokenizer.pad_token

tokenizer.pad_token_id

tokenizer.vocab[tokenizer.pad_token]

pad_index = tokenizer.pad_token_id

test_size = 0.25

train_valid_data = train_data.train_test_split(test_size=test_size)
train_data = train_valid_data["train"]
valid_data = train_valid_data["test"]

train_data = train_data.with_format(type="torch", columns=["ids", "encoded_label"])
valid_data = valid_data.with_format(type="torch", columns=["ids", "encoded_label"])
test_data = test_data.with_format(type="torch", columns=["ids", "encoded_label"])

transformer = transformers.AutoModel.from_pretrained(transformer_name)

transformer.config.hidden_size


class Transformer(nn.Module):
    def __init__(self, transformer, output_dim, freeze):
        super().__init__()
        self.transformer = transformer
        hidden_dim = transformer.config.hidden_size
        self.fc = nn.Linear(hidden_dim, output_dim)

        if freeze:
            for param in self.transformer.parameters():
                param.requires_grad = False

    def forward(self, ids):
        # ids = [batch size, seq len]
        output = self.transformer(ids, output_attentions=True)
        hidden = output.last_hidden_state
        # hidden = [batch size, seq len, hidden dim]
        attention = output.attentions[-1]
        # attention = [batch size, n heads, seq len, seq len]
        cls_hidden = hidden[:, 0, :]
        prediction = self.fc(torch.tanh(cls_hidden))
        # prediction = [batch size, output dim]
        return prediction


output_dim = len(set(train_data["encoded_label"]))
freeze = False

model = Transformer(transformer, output_dim, freeze)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


print(f"The model has {count_parameters(model):,} trainable parameters")

lr = 1e-5

optimizer = optim.Adam(model.parameters(), lr=lr)

criterion = nn.CrossEntropyLoss()

# Check which device is available


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"Device Name: {torch.cuda.get_device_name(device)}")


model = model.to(device)
criterion = criterion.to(device)


def collate(batch, pad_index):
    batch_ids = [i["ids"] for i in batch]
    batch_ids = nn.utils.rnn.pad_sequence(
        batch_ids, padding_value=pad_index, batch_first=True
    )
    batch_label = [i["encoded_label"] for i in batch]
    batch_label = torch.stack(batch_label)
    batch = {"ids": batch_ids, "encoded_label": batch_label}
    return batch


batch_size = 8

collate = functools.partial(collate, pad_index=pad_index)

train_dataloader = torch.utils.data.DataLoader(
    train_data, batch_size=batch_size, collate_fn=collate, shuffle=True
)

valid_dataloader = torch.utils.data.DataLoader(
    valid_data, batch_size=batch_size, collate_fn=collate
)
test_dataloader = torch.utils.data.DataLoader(
    test_data, batch_size=batch_size, collate_fn=collate
)


def train(dataloader, model, criterion, optimizer, device):
    model.train()
    epoch_losses = []
    epoch_accs = []

    for batch in tqdm.tqdm(dataloader, desc="training...", file=sys.stdout):
        ids = batch["ids"].to(device)
        label = batch["encoded_label"].to(device)
        prediction = model(ids)
        loss = criterion(prediction, label)
        accuracy = get_accuracy(prediction, label)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_losses.append(loss.item())
        epoch_accs.append(accuracy.item())

    return epoch_losses, epoch_accs


def evaluate(dataloader, model, criterion, device):
    model.eval()
    epoch_losses = []
    epoch_accs = []

    with torch.no_grad():
        for batch in tqdm.tqdm(dataloader, desc="evaluating...", file=sys.stdout):
            ids = batch["ids"].to(device)
            label = batch["encoded_label"].to(device)
            prediction = model(ids)
            loss = criterion(prediction, label)
            accuracy = get_accuracy(prediction, label)
            epoch_losses.append(loss.item())
            epoch_accs.append(accuracy.item())

    return epoch_losses, epoch_accs


def get_accuracy(prediction, label):
    batch_size, _ = prediction.shape
    predicted_classes = prediction.argmax(dim=-1)
    correct_predictions = predicted_classes.eq(label).sum()
    accuracy = correct_predictions / batch_size
    return accuracy


n_epochs = 3
best_valid_loss = float("inf")

train_losses = []
train_accs = []
valid_losses = []
valid_accs = []

for epoch in range(n_epochs):
    train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)
    valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)

    train_losses.extend(train_loss)
    train_accs.extend(train_acc)
    valid_losses.extend(valid_loss)
    valid_accs.extend(valid_acc)

    epoch_train_loss = np.mean(train_loss)
    epoch_train_acc = np.mean(train_acc)
    epoch_valid_loss = np.mean(valid_loss)
    epoch_valid_acc = np.mean(valid_acc)

    if epoch_valid_loss < best_valid_loss:
        best_valid_loss = epoch_valid_loss
        torch.save(model.state_dict(), "transformer.pt")

    print(f"epoch: {epoch+1}")
    print(f"train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}")
    print(f"valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}")

fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(1, 1, 1)
ax.plot(train_losses, label="train loss")
ax.plot(valid_losses, label="valid loss")
plt.legend()
ax.set_xlabel("updates")
ax.set_ylabel("loss")

fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(1, 1, 1)
ax.plot(train_accs, label="train accuracy")
ax.plot(valid_accs, label="valid accuracy")
plt.legend()
ax.set_xlabel("updates")
ax.set_ylabel("accuracy")

model.load_state_dict(torch.load("transformer.pt"))

test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)

epoch_test_loss = np.mean(test_loss)
epoch_test_acc = np.mean(test_acc)

print(f"test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}")


def predict_sentiment(text, model, tokenizer, device):
    ids = tokenizer(text)["input_ids"]
    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)
    prediction = model(tensor).squeeze(dim=0)
    probability = torch.softmax(prediction, dim=-1)
    predicted_class = prediction.argmax(dim=-1).item()
    predicted_probability = probability[predicted_class].item()
    return predicted_class, predicted_probability


text = "This film is terrible!"

predict_sentiment(text, model, tokenizer, device)

text = "This film is great!"

predict_sentiment(text, model, tokenizer, device)

text = "This film is not terrible, it's great!"

predict_sentiment(text, model, tokenizer, device)

text = "This film is not great, it's terrible!"

predict_sentiment(text, model, tokenizer, device)
